{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8045b3cc-f146-4bdd-9ea2-434816142a65",
   "metadata": {},
   "source": [
    "# Analysis Pipeline Version 0.2.3\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Enter inputs, press `Save Inputs` button, check if the inputs were validated. If not see output message why. Change input and press `Save Inputs` again. If any variables do not apply to your application they should be completley empty.\n",
    "- Press `Run Pipeline`, the progress indicator bar at the bottom indicates how many data subsets have been processed.\n",
    "- The UI will display links to view the images of each data subset in AVIATOR.\n",
    "- **Important:** The input and output directories must both be in one of the following `/home/jovyan/shared/data/` (recommended) or `/home/jovyan/shared/notebooks/` or `/home/jovyan/work/`\n",
    "\n",
    "### Input Descriptions\n",
    "- `inpDir` Full path to the input images, should start with `/home/jovyan/`\n",
    "- `outDir`: Full path to output, should start with `/home/jovyan/`\n",
    "- `logPath`: Full path to this dashboard log outputs\n",
    "- `filePattern` The file pattern of the input images (Include ALL variables)\n",
    "- `metaDir` The full path to the metadata directory, should start with `/home/jovyan/`\n",
    "- `workflowName` This is just the name of the directory to place the final output on the file server. The path will be `/home/jovyan/shared/data/file-server/pyramids/visualization-pipeline/<workflowName>` - Note you cannot create a directory which already exists or we could accidentally save over someone else's work. This UI will check to make sure that the same name is not already taken.\n",
    "- `subsetBy` The variable used to subset the data. For instance, you might process each plate individually to have final summary statistics for each plate.\n",
    "- `groupBy` These are the variables to group together in final summary statistics calculations. For example, you might choose the two variables which represent each individual well.\n",
    "- `maskChannel` This is the channel (including variable and number, i.e. `c1`) which will be used to segment the channel of interest. This might be your cell nuclei channel, for example.\n",
    "- `interestChannel` This is the channel (including variable and number, i.e. `c2`) which you are interested in calculating final summary statistics for. This might be your reporter channel.\n",
    "- `subsets`: Which subsets of the dataset should be processed. For example, if the `subsetBy` variable is plate number, then you can process the first 3 plates by setting this to `1,2,3`. Set this to `all` to process the entire dataset.\n",
    "- `maxObjectArea`: Objects with an area greater than this are removed from the analysis. The area is measured in squared pixels.\n",
    "\n",
    "\n",
    "### Version Notes\n",
    "This version supports parallel execution. Each worker is responsible for starting the workflow (for a given subset) and monitoring its containers so that it can start the next container in the workflow upon completion of the previous container. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57d406de-47af-4d22-b1e9-f93c049a08b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3b4036ff914b68b241168de2398c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import re\n",
    "import subprocess\n",
    "import ipywidgets as widgets\n",
    "import multiprocessing as mp\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from IPython.display import display\n",
    "from filepattern import FilePattern\n",
    "from multiprocessing import Pool\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def parallelize_workflow(map_args, ui, log_path):\n",
    "    hashes = []\n",
    "    num_workers = min(int(mp.cpu_count()//2), 4)\n",
    "    with ui.out:\n",
    "        print('Number of workers = {}'.format(num_workers))\n",
    "        print('Spawning workers...')\n",
    "        complete = 0\n",
    "\n",
    "        with Pool(processes=num_workers) as pool:\n",
    "            for status, subset, subset_hashes in pool.imap(run_workflow, map_args):\n",
    "                hashes.extend(subset_hashes)\n",
    "                with open(log_path, 'a') as fw:\n",
    "                    if status:\n",
    "                        fw.write('{} : subset {} successful : hashes : {}\\n'.format(datetime.now(), subset, ' '.join(subset_hashes)))\n",
    "\n",
    "                        # Stop the successful containers upon completion\n",
    "                        stopped_containers = stop_containers(subset_hashes)\n",
    "                        print('Stopped containers of subset {} : hashes : {}'.format(subset, ' '.join(stopped_containers)))\n",
    "\n",
    "                    else:\n",
    "                        fw.write('ERROR : {} : subset {} failed : hashes : {}\\n'.format(datetime.now(), subset, ' '.join(subset_hashes)))\n",
    "\n",
    "                    fw.close()\n",
    "\n",
    "                complete += 1\n",
    "                ui.progress_bar.value = complete\n",
    "\n",
    "    return hashes\n",
    "\n",
    "\n",
    "def check_container(container_hash):\n",
    "    \"\"\"\n",
    "    Checks the status of a docker container.\n",
    "    \"\"\"\n",
    "    cmd = 'docker ps'\n",
    "    out = subprocess.check_output(cmd, shell=True).decode(sys.stdout.encoding).split('\\n')\n",
    "    containers = [c.split() for c in out]\n",
    "    status = 'Unknown'\n",
    "    # with self.out:\n",
    "    #     print(containers)\n",
    "    for c in containers:\n",
    "        if container_hash in c:\n",
    "            # print(c)\n",
    "            status = c[-2]\n",
    "            break\n",
    "    return status\n",
    "\n",
    "\n",
    "def run_container(args, volume):\n",
    "    \"\"\"\n",
    "    Runs a docker container.\n",
    "    \"\"\"\n",
    "    dock8r = 'docker run -v'\n",
    "\n",
    "    jvolume, dvolume = volume.split(':')\n",
    "\n",
    "    container = args['container']\n",
    "    plugin_args = [\n",
    "        '--' + arg + ' ' + str(value)\n",
    "        for arg, value in args.items() if arg != 'container' and value != ''\n",
    "    ]\n",
    "    for i, arg in enumerate(plugin_args):\n",
    "        if jvolume in arg:\n",
    "            plugin_args[i] = arg.replace(jvolume, dvolume)\n",
    "\n",
    "    cmd_list = [dock8r] + [volume] + [container] + plugin_args\n",
    "    cmd = ' '.join(cmd_list)\n",
    "\n",
    "    full_hash = subprocess.check_output(cmd, shell=True).decode(sys.stdout.encoding).strip()\n",
    "    container_hash = full_hash.split('-')[-1]\n",
    "    print('Running {} hash:{}'.format(container, container_hash))\n",
    "\n",
    "    status = 'Unknown'\n",
    "    i = 0\n",
    "\n",
    "    while status != 'Succeeded':\n",
    "        sleep(5)\n",
    "        status = check_container(container_hash)\n",
    "        print('Status of Container {} is {} : {}'.format(container_hash[-6:], status, datetime.now()))\n",
    "        i += 1\n",
    "        if i > 20 and status == 'Pending':\n",
    "            raise Exception('Docker contianer {} failed to start'.format(container_hash))\n",
    "\n",
    "        if i > 10 and status == 'Unknown':\n",
    "            raise Exception('Failed to determine the status of container {}'.format(container_hash))\n",
    "\n",
    "        if status == 'Failed':\n",
    "            raise Exception('Docker contianer {} has failed'.format(container_hash))\n",
    "\n",
    "    return 'Succeeded', container_hash\n",
    "\n",
    "\n",
    "def run_workflow(plugin_args):\n",
    "    \"\"\"\n",
    "    Controls the visualization workflow for a data subset.\n",
    "    \"\"\"\n",
    "\n",
    "    hashes = []\n",
    "    volume = plugin_args['volume']\n",
    "    subset = plugin_args['subset']\n",
    "    run_flatfield = plugin_args['run_flatfield']\n",
    "\n",
    "    try:\n",
    "\n",
    "        if run_flatfield:\n",
    "            print('Running Basic Flatfield Plugin on subset {}'.format(subset))\n",
    "            # Run the basic flatfiled for specified subset\n",
    "            basic_flatfield_args = plugin_args['BasicFlatfieldCorrectionPlugin']\n",
    "            basic_flatfield_args['filePattern'] = basic_flatfield_args['filePattern'][subset]\n",
    "            basic_flatfield_args['outDir'] = basic_flatfield_args['outDir'][subset]\n",
    "            # pprint(basic_flatfield_args)\n",
    "            status, container_hash = run_container(basic_flatfield_args, volume)\n",
    "            hashes.append(container_hash)\n",
    "            if status != 'Succeeded':\n",
    "                raise Exception('Status of container {} is {}'.format(\n",
    "                    container_hash, status))\n",
    "\n",
    "            print('Running Apply Flatfield Plugin on subset {}'.format(subset))\n",
    "            # Run the apply flatfield plugin for specified subset\n",
    "            apply_flatfield_args = plugin_args['ApplyFlatfield']\n",
    "            apply_flatfield_args['imgPattern'] = apply_flatfield_args['imgPattern'][subset]\n",
    "            apply_flatfield_args['ffDir'] = basic_flatfield_args['outDir']\n",
    "            apply_flatfield_args['brightPattern'] = apply_flatfield_args['brightPattern'][subset]\n",
    "            apply_flatfield_args['darkPattern'] = apply_flatfield_args['darkPattern'][subset]\n",
    "            apply_flatfield_args['outDir'] = apply_flatfield_args['outDir'][subset]\n",
    "            # pprint(apply_flatfield_args)\n",
    "            status, container_hash = run_container(apply_flatfield_args, volume)\n",
    "            hashes.append(container_hash)\n",
    "            if status != 'Succeeded':\n",
    "                raise Exception('Status of container {} is {}'.format(\n",
    "                    container_hash, status))\n",
    "\n",
    "        else:\n",
    "            basic_flatfield_args = plugin_args['BasicFlatfieldCorrectionPlugin']\n",
    "            apply_flatfield_args = plugin_args['ApplyFlatfield']\n",
    "            apply_flatfield_args['outDir'] = basic_flatfield_args['inpDir']\n",
    "\n",
    "        print('Running SMP Training Plugin on subset {}'.format(subset))\n",
    "        # Run the apply flatfield plugin for specified subset\n",
    "        smp_args = plugin_args['DemoSmpTraining_Inference']\n",
    "        smp_args['imagesInferenceDir'] = apply_flatfield_args['outDir']\n",
    "        smp_args['inferencePattern'] = smp_args['inferencePattern'][subset]\n",
    "        smp_args['outputDir'] = smp_args['outputDir'][subset]\n",
    "        # pprint(smp_args)\n",
    "        status, container_hash = run_container(smp_args, volume)\n",
    "        hashes.append(container_hash)\n",
    "        if status != 'Succeeded':\n",
    "            raise Exception('Status of container {} is {}'.format(\n",
    "                    container_hash, status))\n",
    "\n",
    "\n",
    "        print('Running FTL Label Plugin on subset {}'.format(subset))\n",
    "        # Run the apply flatfield plugin for specified subset\n",
    "        ftl_args = plugin_args['FtlLabel']\n",
    "        ftl_args['inpDir'] = smp_args['outputDir']\n",
    "        ftl_args['outDir'] = ftl_args['outDir'][subset]\n",
    "        # pprint(ftl_args)\n",
    "        status, container_hash = run_container(ftl_args, volume)\n",
    "        hashes.append(container_hash)\n",
    "        if status != 'Succeeded':\n",
    "            raise Exception('Status of container {} is {}'.format(\n",
    "                    container_hash, status))\n",
    "\n",
    "\n",
    "        print('Running Binary Operations Plugin on subset {}'.format(subset))\n",
    "        # Run the binary operations plugin to remove large objects on subset\n",
    "        binary_args = plugin_args['BinaryOperationsPlugin']\n",
    "        binary_args['inpDir'] = ftl_args['outDir'][subset]\n",
    "        binary_args['filePattern'] = binary_args['filePattern'][subset]\n",
    "        binary_args['outDir'] = binary_args['outDir'][subset]\n",
    "        status, container_hash = run_container(binary_args, volume)\n",
    "        hashes.append(container_hash)\n",
    "        if status != 'Succeeded':\n",
    "            raise Exception('Status of container {} is {}'.format(\n",
    "                    container_hash, status))\n",
    "\n",
    "        print('Running Nyxus Plugin on subset {}'.format(subset))\n",
    "        # Run the apply flatfield plugin for specified subset\n",
    "        nyxus_args = plugin_args['NyxusPlugin']\n",
    "        nyxus_args['inpDir'] = apply_flatfield_args['outDir']\n",
    "        nyxus_args['segDir'] = ftl_args['outDir']\n",
    "        nyxus_args['outDir'] = nyxus_args['outDir'][subset]\n",
    "        # pprint(nyxus_args)\n",
    "        status, container_hash = run_container(nyxus_args, volume)\n",
    "        hashes.append(container_hash)\n",
    "        if status != 'Succeeded':\n",
    "            raise Exception('Status of container {} is {}'.format(\n",
    "                    container_hash, status))\n",
    "\n",
    "\n",
    "        print('Running Threshold Plugin on subset {}'.format(subset))\n",
    "        # Run the apply flatfield plugin for specified subset\n",
    "        threshold_args = plugin_args['TabularThresholdingPlugin']\n",
    "        threshold_args['inpDir'] = nyxus_args['outDir']\n",
    "        threshold_args['outDir'] = threshold_args['outDir'][subset]\n",
    "        # pprint(threshold_args)\n",
    "        status, container_hash = run_container(threshold_args, volume)\n",
    "        hashes.append(container_hash)\n",
    "        if status != 'Succeeded':\n",
    "            raise Exception('Status of container {} is {}'.format(\n",
    "                    container_hash, status))\n",
    "\n",
    "\n",
    "        print('Running CSV Statistics Plugin on subset {}'.format(subset))\n",
    "        # Run the apply flatfield plugin for specified subset\n",
    "        csv_args = plugin_args['CsvStatistics']\n",
    "        csv_args['inpDir'] = threshold_args['outDir']\n",
    "        csv_args['filePattern'] = csv_args['filePattern'][subset]\n",
    "        csv_args['outDir'] = csv_args['outDir'][subset]\n",
    "        # pprint(csv_args)\n",
    "        status, container_hash = run_container(csv_args, volume)\n",
    "        hashes.append(container_hash)\n",
    "        if status != 'Succeeded':\n",
    "            raise Exception('Status of container {} is {}'.format(\n",
    "                    container_hash, status))\n",
    "\n",
    "        return True, subset, hashes\n",
    "\n",
    "    except Exception:\n",
    "        print('Subset {} failed'.format(subset))\n",
    "        return False, subset, hashes\n",
    "\n",
    "\n",
    "def common_parent(paths):\n",
    "    \"\"\"\n",
    "    Returns the deepest common parent of 2 paths.\n",
    "    \"\"\"\n",
    "    parents = [\n",
    "        [parent for parent in Path(path).parents][:-1]\n",
    "        for path in paths]\n",
    "\n",
    "    for parent_list in parents:\n",
    "        parent_list.reverse()\n",
    "\n",
    "    common = None\n",
    "\n",
    "    for level in zip(*parents):\n",
    "        if len(set(level)) == 1:\n",
    "            common = level[0]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return common\n",
    "\n",
    "def stop_containers(hashes):\n",
    "    \"\"\"\n",
    "    Stops all specified containers.\n",
    "    \"\"\"\n",
    "    stopped_containers = []\n",
    "    for h in hashes:\n",
    "        cmd = 'docker stop {}'.format(h)\n",
    "        stopped_containers.append(\n",
    "            subprocess.check_output(\n",
    "                cmd, shell=True\n",
    "            ).decode(sys.stdout.encoding).strip()\n",
    "        )\n",
    "        # print('Stopped container {}'.format(h))\n",
    "\n",
    "    return stopped_containers\n",
    "\n",
    "\n",
    "def parse_log(log_path):\n",
    "\n",
    "    re_subset = re.compile(r'(?<=subset\\s)[0-9]+')\n",
    "    success = {}\n",
    "\n",
    "    with open(log_path) as fr:\n",
    "        for line in fr.readlines():\n",
    "\n",
    "            subset = int(re_subset.search(line).group(0))\n",
    "\n",
    "            if 'ERROR' in line:\n",
    "                success[subset] = False\n",
    "\n",
    "            else:\n",
    "                success[subset] = True\n",
    "\n",
    "    failed_subsets = [subset for subset, status in success.items() if not status]\n",
    "\n",
    "    return failed_subsets\n",
    "\n",
    "\n",
    "def parse_output(outDir):\n",
    "\n",
    "    complete = []\n",
    "    incomplete = []\n",
    "    precompute_out = outDir.joinpath('CsvStatistics')\n",
    "\n",
    "    for subset in precompute_out.iterdir():\n",
    "\n",
    "        s = int(''.join([i for i in str(subset.name) if i.isnumeric()]))\n",
    "\n",
    "        if list(subset.iterdir()):\n",
    "            complete.append(s)\n",
    "\n",
    "        else:\n",
    "            incomplete.append(s)\n",
    "\n",
    "    complete.sort()\n",
    "    incomplete.sort()\n",
    "\n",
    "    return complete, incomplete\n",
    "\n",
    "\n",
    "class Visualization_UI:\n",
    "    \"\"\"\n",
    "    A class to display a UI of the visualization pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Define dashboard style and stdout\n",
    "        self.layout = widgets.Layout(width='auto', height='40px')\n",
    "        self.style = {'description_width': 'initial'}\n",
    "        self.out = widgets.Output(layout={'border': '1px solid black'})\n",
    "\n",
    "        # Define variable to track dashboard workflow\n",
    "        self.running = False\n",
    "        self.valid_inputs = False\n",
    "\n",
    "        self.hashes = []\n",
    "\n",
    "        # Define the pipeline plugin workflow\n",
    "        self.workflow = {\n",
    "            'BasicFlatfieldCorrectionPlugin': None,\n",
    "            'ApplyFlatfield': None,\n",
    "            'DemoSmpTraining_Inference': None,\n",
    "            'FtlLabel': None,\n",
    "            'BinaryOperationsPlugin': None,\n",
    "            'NyxusPlugin': None,\n",
    "            'TabularThresholdingPlugin': None,\n",
    "            'CsvStatistics': None\n",
    "        }\n",
    "\n",
    "        # Define the user arguments required for the pipeline\n",
    "        self.user_args = {\n",
    "            'inpDir': '/home/jovyan/shared/data/eastman_subset/input',\n",
    "            'outDir': '/home/jovyan/shared/data/eastman_subset/test-analysis-v030/',\n",
    "            'logPath': '/home/jovyan/shared/notebooks/pipelines/analysis/logs/test-analysis-v030.log',\n",
    "            'filePattern': 'p{ppp}_x{xx}_y{yy}_wx{t}_wy{z}_c{c}.ome.tif',\n",
    "            'metaDir': '/home/jovyan/shared/data/eastman_subset/meta',\n",
    "            'subsetBy': 'p',\n",
    "            'groupBy': 'tz',\n",
    "            'maskChannel': 'c1',\n",
    "            'interestChannel': 'c2',\n",
    "            'subsets': 'all',\n",
    "            'maxObjectArea': '500'\n",
    "        }\n",
    "\n",
    "        # Define the pipeline's user input widgets\n",
    "        self.arg_widgets = [\n",
    "            widgets.Text(\n",
    "                value=self.user_args[f],\n",
    "                description=f,\n",
    "                layout=self.layout,\n",
    "                style=self.style\n",
    "            )\n",
    "            for f in self.user_args.keys()\n",
    "        ]\n",
    "\n",
    "        # Define ability for user to run flatfield or not\n",
    "        self.run_flatfield = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description='Run Flatfield Correction',\n",
    "            disabled=False\n",
    "        )\n",
    "\n",
    "        # Define center fov flatfield correction button\n",
    "        self.center_fov = widgets.RadioButtons(\n",
    "            options=['normal', 'center fov'],\n",
    "            value='normal',\n",
    "            description='Flatfield Correction:',\n",
    "            disabled=False,\n",
    "            layout=self.layout,\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "\n",
    "        # Define the save inputs button\n",
    "        self.save_args = widgets.Button(\n",
    "            value=False,\n",
    "            description='Save Inputs',\n",
    "            disabled=False,\n",
    "            button_style='',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "            tooltip='Save the Inputs',\n",
    "            icon='save'  # (FontAwesome names without the `fa-` prefix)\n",
    "        )\n",
    "\n",
    "        self.run_pipeline = widgets.Button(\n",
    "            value=True,\n",
    "            description='Run Pipeline',\n",
    "            disabled=False,\n",
    "            button_style='',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "            tooltip='Runs the pipeline with current inputs (make sure to save inputs first)',\n",
    "            icon='play'  # (FontAwesome names without the `fa-` prefix)\n",
    "        )\n",
    "\n",
    "        self.resume_pipeline = widgets.Button(\n",
    "            value=True,\n",
    "            description='Resume Pipeline',\n",
    "            disabled=False,\n",
    "            button_style='',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "            tooltip='Resumes the pipeline with current inputs (make sure to save inputs first)',\n",
    "            icon='step-forward'  # (FontAwesome names without the `fa-` prefix)\n",
    "        )\n",
    "\n",
    "        self.progress_bar = widgets.IntProgress(\n",
    "            value=0,\n",
    "            min=0,\n",
    "            max=None,\n",
    "            description='Subsets Processed:',\n",
    "            style=dict(description_width='initial'),\n",
    "            layout=self.layout\n",
    "            )\n",
    "\n",
    "        # Define the method to run on button clicks\n",
    "        self.save_args.on_click(self.on_save_clicked)\n",
    "        self.run_pipeline.on_click(self.on_run_clicked)\n",
    "\n",
    "        self.w = [\n",
    "            *self.arg_widgets,\n",
    "            self.run_flatfield,\n",
    "            self.center_fov,\n",
    "            self.save_args,\n",
    "            self.run_pipeline,\n",
    "            self.resume_pipeline,\n",
    "            self.progress_bar,\n",
    "            self.out\n",
    "        ]\n",
    "\n",
    "    def display_widgets(self):\n",
    "        \"\"\"\n",
    "        A method to display the widgets\n",
    "        \"\"\"\n",
    "\n",
    "        display(\n",
    "            *self.w\n",
    "        )\n",
    "\n",
    "    def on_save_clicked(self, b):\n",
    "        \"\"\"\n",
    "        A method to save the users inputs.\n",
    "        \"\"\"\n",
    "        with self.out:\n",
    "            print('Saving args...')\n",
    "            for w in self.arg_widgets:\n",
    "                arg = w.description\n",
    "                value = w.value\n",
    "                self.user_args[arg] = value\n",
    "                print('{} = {}'.format(arg, value))\n",
    "\n",
    "            if self.run_flatfield.value:\n",
    "                if self.center_fov.value == 'normal':\n",
    "                    print('You have selected to run a normal Flatfield Correction')\n",
    "                else:\n",
    "                    print('You have selected to run a center FOV Flatfield Correction')\n",
    "\n",
    "            else:\n",
    "                print('You have selected to NOT run Flatfield Correction')\n",
    "\n",
    "            print('Validating inputs...')\n",
    "            self.valid_inputs = self.validate_inputs()\n",
    "\n",
    "    # TODO: Add more input validation checks\n",
    "    def validate_inputs(self):\n",
    "        \"\"\"\n",
    "        A method to validate the user's inputs.\n",
    "        \"\"\"\n",
    "        # with self.out:\n",
    "        inpDir = Path(self.user_args['inpDir'])\n",
    "\n",
    "        # Validate inpDir\n",
    "        if not inpDir.exists():\n",
    "            print('\\n***WARNING***\\nThe inpDir {} does not exist'.format(inpDir))\n",
    "            return False\n",
    "\n",
    "        maskVar = re.sub('[0-9]', '', self.user_args['maskChannel'])\n",
    "        intVar = re.sub('[0-9]', '', self.user_args['interestChannel'])\n",
    "        if maskVar != intVar:\n",
    "            print('The mask channel and channel of interest must use the same variable')\n",
    "            return False\n",
    "\n",
    "        print('Inputs validated!')\n",
    "        return True\n",
    "\n",
    "    def on_run_clicked(self, b):\n",
    "        \"\"\"\n",
    "        A method to start the pipeline on click\n",
    "        \"\"\"\n",
    "\n",
    "        if self.running:\n",
    "            with self.out:\n",
    "                print('Pipeline already running, you must kill the pipeline to restart')\n",
    "\n",
    "        elif not self.valid_inputs:\n",
    "            with self.out:\n",
    "                print('The inputs must be validated before running the pipeline, click Save Inputs to validate')\n",
    "\n",
    "        else:\n",
    "            self.running = True\n",
    "\n",
    "            with self.out:\n",
    "                print('Starting the pipeline...')\n",
    "                print('Inferring plugin inputs and creating output file locations')\n",
    "                self.generate_plugin_inputs()\n",
    "                print('Running containers...')\n",
    "\n",
    "            self.subsets = [i for i in range(len(self.filePatterns))]\n",
    "            map_args = [self.plugin_args.copy() for subset in self.subsets]\n",
    "\n",
    "            for subset in self.subsets:\n",
    "                map_args[subset]['subset'] = subset\n",
    "\n",
    "            if self.user_args['subsets'] != 'all':\n",
    "                to_process = [int(s) for s in self.user_args['subsets'].split(',')]\n",
    "                map_args = [map_args[s] for s in to_process]\n",
    "\n",
    "            self.progress_bar.max = len(map_args)\n",
    "            log_path = self.user_args['logPath']\n",
    "\n",
    "            self.hashes.extend(parallelize_workflow(map_args, self, log_path))\n",
    "\n",
    "            self.running = False\n",
    "\n",
    "    def on_resumed_clicked(self, b):\n",
    "        \"\"\"\n",
    "        A method to resume the pipeline\n",
    "        \"\"\"\n",
    "\n",
    "        out_dir = Path(self.user_args['outDir'])\n",
    "\n",
    "        with self.out:\n",
    "            assert out_dir.exists(), 'The output directory for resuming a pipeline must already exist'\n",
    "\n",
    "        if self.running:\n",
    "            with self.out:\n",
    "                print('Pipeline already running, you must kill the pipeline to restart')\n",
    "\n",
    "        elif not self.valid_inputs:\n",
    "            with self.out:\n",
    "                print('The inputs must be validated before running the pipeline, click Save Inputs to validate')\n",
    "\n",
    "        else:\n",
    "            self.running = True\n",
    "\n",
    "            with self.out:\n",
    "                log_path = self.user_args['logPath']\n",
    "                print('Parsing {}'.format(out_dir))\n",
    "\n",
    "                # complete_subsets, failed_subsets = parse_log(log_path)\n",
    "                complete_subsets, failed_subsets = parse_output(out_dir)\n",
    "\n",
    "                print('Inferring plugin inputs and output file locations...')\n",
    "                self.generate_plugin_inputs(resume=True)\n",
    "\n",
    "                map_args = []\n",
    "                self.subsets = []\n",
    "\n",
    "                if self.user_args['subsets'] != 'all':\n",
    "                    user_subsets = [int(s) for s in self.user_args['subsets'].split(',')]\n",
    "\n",
    "                for i in range(len(self.filePatterns)):\n",
    "                    if i not in complete_subsets:\n",
    "                        if self.user_args['subsets'] == 'all' or i in user_subsets:\n",
    "                            plugin_args = self.plugin_args.copy()\n",
    "                            plugin_args['subset'] = i\n",
    "                            map_args.append(plugin_args)\n",
    "                            self.subsets.append(i)\n",
    "\n",
    "                self.progress_bar.max = len(map_args)\n",
    "                log_path = self.user_args['logPath']\n",
    "\n",
    "                print('Resuming subsets : {}'.format(self.subsets))\n",
    "                print('Starting containers...')\n",
    "                self.hashes.extend(parallelize_workflow(map_args, self, log_path))\n",
    "                self.running = False\n",
    "\n",
    "    def generate_outdir(self, resume):\n",
    "\n",
    "        self.outDir = Path(self.user_args['outDir'])\n",
    "\n",
    "        # Make the out directory\n",
    "        if not resume:\n",
    "            os.mkdir(self.outDir)\n",
    "\n",
    "        flatfield_plugins = [\n",
    "            'BasicFlatfieldCorrectionPlugin', 'ApplyFlatfield'\n",
    "        ]\n",
    "\n",
    "        # Make plugin out directories\n",
    "        for plugin in self.workflow:\n",
    "\n",
    "            if plugin in flatfield_plugins and not self.run_flatfield.value:\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                sub_dir = self.outDir.joinpath(plugin)\n",
    "\n",
    "            if not resume:\n",
    "                os.mkdir(sub_dir)\n",
    "\n",
    "            self.workflow[plugin] = []\n",
    "\n",
    "            # For each data subset create a subset directory\n",
    "            for i in range(len(self.filePatterns)):\n",
    "\n",
    "                subset_dir = sub_dir.joinpath('subset{}'.format(str(i)))\n",
    "                self.workflow[plugin].append(subset_dir)\n",
    "\n",
    "                if resume:\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    os.mkdir(subset_dir)\n",
    "\n",
    "    # TODO: Assemble patterns should be generated from actual file list\n",
    "    def generate_assemble_pattern(self, groupBy):\n",
    "        \"\"\"\n",
    "        Generates assembled patterns, similar to a vector pattern\n",
    "        \"\"\"\n",
    "        self.assemblePatterns = self.filePatterns.copy()\n",
    "\n",
    "        # Iterate over the groupBy variables\n",
    "        for v in groupBy:\n",
    "            # Define the re pattern\n",
    "            re_pattern = re.compile(r'\\{(' + v + r'+)\\}')\n",
    "            # Define the min and max unique values of the variable\n",
    "            # with self.out:\n",
    "            # print(v)\n",
    "            v_unique = self.fp.uniques[v]\n",
    "            # print(v_unique)\n",
    "            v_min = min(v_unique)\n",
    "            v_max = max(v_unique)\n",
    "            num_dig = len(str(v_max))\n",
    "            # Define the pattern to sub in for the variable\n",
    "            v_pattern = '(' + str(v_min).zfill(num_dig) + '-' + str(v_max) + ')'\n",
    "\n",
    "            # Iterate over all file patterns (one for each data subset)\n",
    "            for i, pattern in enumerate(self.assemblePatterns):\n",
    "                assemblePattern = re_pattern.sub(v_pattern, pattern)\n",
    "                self.assemblePatterns[i] = assemblePattern\n",
    "\n",
    "        self.assemblePatterns = ['\"' + p + '\"' for p in self.assemblePatterns]\n",
    "\n",
    "    def generate_smp_patterns(self):\n",
    "        \"\"\"Generates smp file patterns for each data subset\"\"\"\n",
    "\n",
    "        mask_channel = self.user_args['maskChannel']\n",
    "        channel_var = re.sub('[0-9]', '', mask_channel)\n",
    "\n",
    "        pattern = re.compile(\n",
    "            channel_var + '\\{(' + channel_var + '+|' + channel_var + '\\+)\\}')\n",
    "\n",
    "        smp_file_patterns = [\n",
    "            pattern.sub(mask_channel, fp) for fp in self.filePatterns\n",
    "        ]\n",
    "\n",
    "        return smp_file_patterns\n",
    "\n",
    "    def generate_nyxus_pattern(self):\n",
    "\n",
    "        subset_var = self.user_args['subsetBy']\n",
    "        mask_channel = self.user_args['maskChannel']\n",
    "        channel_var = re.sub('[0-9]', '', mask_channel)\n",
    "        file_pattern = self.user_args['filePattern']\n",
    "\n",
    "        for c in file_pattern:\n",
    "            if c == subset_var:\n",
    "                nyxus_pattern = subset_var + '{' + subset_var + '+}' + '.*' + mask_channel + '.*'\n",
    "\n",
    "                return nyxus_pattern\n",
    "\n",
    "            elif c == channel_var:\n",
    "                nyxus_pattern = mask_channel + '.*' + subset_var + '{' + subset_var + '+}' + '.*'\n",
    "\n",
    "                return nyxus_pattern \n",
    "\n",
    "    def generate_plugin_inputs(self, resume=False):\n",
    "\n",
    "        self.fp = FilePattern(\n",
    "            self.user_args['inpDir'], self.user_args['filePattern']\n",
    "        )\n",
    "\n",
    "        flatfield_pattern = self.user_args['filePattern']\n",
    "\n",
    "        if self.center_fov.value != 'normal':\n",
    "            # Find range of well variables\n",
    "            for v in self.user_args['wellVar']:\n",
    "                # Define the regular expression to parse the well variables\n",
    "                well_re = re.compile(r'\\{(' + v + r'+)\\}')\n",
    "                uniques = self.fp.uniques[v]\n",
    "                idx = len(uniques)//2\n",
    "                flatfield_pattern = well_re.sub(str(uniques[idx]), flatfield_pattern)\n",
    "\n",
    "        if self.user_args['subsetBy']:\n",
    "\n",
    "            # Define the variable which the data will be split\n",
    "            v = self.user_args['subsetBy']\n",
    "\n",
    "            # Define the regular expression to parse the subset variable\n",
    "            subset_re = re.compile(r'\\{(' + v + r'+)\\}')\n",
    "            pattern = self.user_args['filePattern']\n",
    "\n",
    "            # Find the subset variable and its length\n",
    "            subset_var = subset_re.search(pattern).group(1)\n",
    "            subset_len = len(subset_var)\n",
    "\n",
    "            # Get the subset images\n",
    "            # subsets = self.subset_images()\n",
    "\n",
    "            # Generate the filepatterns for each data subset\n",
    "            self.filePatterns = [\n",
    "                subset_re.sub(str(value).zfill(subset_len), pattern)\n",
    "                for value in self.fp.uniques[v]\n",
    "            ]\n",
    "\n",
    "            self.flatfieldPatterns = [\n",
    "                subset_re.sub(str(value).zfill(subset_len), flatfield_pattern)\n",
    "                for value in self.fp.uniques[v]\n",
    "            ]\n",
    "\n",
    "            self.filePatterns.sort()\n",
    "            self.flatfieldPatterns.sort()\n",
    "            groupBy = self.fp.variables.replace(v, '')\n",
    "\n",
    "        else:\n",
    "            self.filePatterns = [self.fp]\n",
    "            groupBy = self.fp.variables\n",
    "\n",
    "        channelVar = re.sub(r'[0-9]', '', self.user_args['maskChannel'])\n",
    "        groupBy = groupBy.replace(channelVar, '')\n",
    "\n",
    "        with self.out:\n",
    "            print('Generating assembled file patterns')\n",
    "        # Generate the assembled patterns for each subset\n",
    "        self.generate_assemble_pattern(groupBy)\n",
    "\n",
    "        with self.out:\n",
    "            print('Generating output directories')\n",
    "        self.generate_outdir(resume)\n",
    "\n",
    "        # Define the container volumes\n",
    "        pre_trained_model_dir = '/home/jovyan/shared/data/eastman_subset/UnetPlusPlus-MCCLoss-efficientnet-b6-imagenet-Adam/'\n",
    "        meta_dir = self.user_args['metaDir']\n",
    "        jvolume = str(common_parent([ui.outDir, pre_trained_model_dir, meta_dir]))\n",
    "        dvolume = jvolume.replace('/home/jovyan', '/opt')\n",
    "        self.volume = jvolume + ':' + dvolume\n",
    "\n",
    "        brightPatterns = [pattern.replace('.ome.tif', '_flatfield.ome.tif')\n",
    "                    for pattern in self.assemblePatterns]\n",
    "\n",
    "        darkPatterns = [pattern.replace('.ome.tif', '_darkfield.ome.tif')\n",
    "                        for pattern in self.assemblePatterns]\n",
    "\n",
    "        # Generate the smp-training filePatterns\n",
    "        smp_file_patterns = self.generate_smp_patterns()\n",
    "\n",
    "        nyxus_pattern = self.generate_nyxus_pattern()\n",
    "\n",
    "        # Define the csv filePatterns\n",
    "        mask = self.user_args['maskChannel']\n",
    "        reporter = self.user_args['interestChannel']\n",
    "        csv_file_patterns = [\n",
    "            fp.replace(mask, reporter) for fp in smp_file_patterns\n",
    "            ]\n",
    "\n",
    "        # Define the plugin inputs\n",
    "        self.basic_flatfield_args = {\n",
    "            'container': 'polusai/basic-flatfield-correction-plugin:1.3.1',\n",
    "            'inpDir': self.user_args['inpDir'],\n",
    "            'filePattern': self.flatfieldPatterns,\n",
    "            'darkfield': True,\n",
    "            'photobleach': True,\n",
    "            'groupBy': groupBy,\n",
    "            'outDir': self.workflow['BasicFlatfieldCorrectionPlugin']\n",
    "        }\n",
    "\n",
    "        self.apply_flatfield_args = {\n",
    "            'container': 'polusai/apply-flatfield-plugin:1.2.0',\n",
    "            'imgDir': self.user_args['inpDir'],\n",
    "            'imgPattern': self.filePatterns,\n",
    "            'brightPattern': brightPatterns,\n",
    "            'darkPattern': darkPatterns,\n",
    "            'outDir': self.workflow['ApplyFlatfield']\n",
    "        }\n",
    "\n",
    "        self.smp_args = {\n",
    "            'container': 'polusai/smp-training-plugin:0.5.10',\n",
    "            'inferenceMode': 'active',\n",
    "            'inferencePattern': smp_file_patterns,\n",
    "            'pretrainedModel': pre_trained_model_dir,\n",
    "            'batchSize': '10',\n",
    "            'device': 'gpu',\n",
    "            'lossName': 'MCCLoss',\n",
    "            'checkpointFrequency': '10',\n",
    "            'maxEpochs': '10',\n",
    "            'patience': '10',\n",
    "            'minDelta': '10.0',\n",
    "            'outputDir': self.workflow['DemoSmpTraining_Inference']\n",
    "        }\n",
    "\n",
    "        self.ftl_args = {\n",
    "            'container': 'polusai/ftl-label-plugin:0.3.11',\n",
    "            'connectivity': '1',\n",
    "            'binarizationThreshold': '0.5',\n",
    "            'outDir': self.workflow['FtlLabel']\n",
    "        }\n",
    "\n",
    "        self.binary_args = {\n",
    "            'container': 'polusai/binary-operations-plugin:0.5.0-dev0',\n",
    "            'filePattern': self.filePatterns,\n",
    "            'operation': 'removeLarge',\n",
    "            'threshold': self.user_args['maxObjectArea'],\n",
    "            'outDir': self.workflow['BinaryOperationsPlugin']\n",
    "        }\n",
    "\n",
    "        self.nyxus_args = {\n",
    "            'container': 'polusai/nyxus-plugin:0.1.2',\n",
    "            'features': 'BASIC_MORPHOLOGY,ALL_INTENSITY',\n",
    "            'filePattern': nyxus_pattern, # *p{p+}*c1*\n",
    "            'mapVar': reporter,\n",
    "            'neighborDist': '5',\n",
    "            'pixelPerMicron': '0.83612',\n",
    "            'outDir': self.workflow['NyxusPlugin']\n",
    "        }\n",
    "\n",
    "        self.threshold_args = {\n",
    "            'container': 'polusai/tabular-data-thresholding:0.1.0',\n",
    "            'metaDir': meta_dir,\n",
    "            'mappingvariableName': 'intensity_image',\n",
    "            'negControl': 'virus_negative',\n",
    "            'posControl': 'virus_neutral',\n",
    "            'variableName': 'MEAN',\n",
    "            'thresholdType': 'all',\n",
    "            'numBins': '512',\n",
    "            'falsePositiverate': '0.1',\n",
    "             # threshold.n: '4'\n",
    "            'outFormat': 'csv',\n",
    "            'outDir': self.workflow['TabularThresholdingPlugin'],\n",
    "        }\n",
    "\n",
    "        self.csv_args = {\n",
    "            'container': 'polusai/csv-statistics-plugin:0.2.1',\n",
    "            'statistics': 'mean',\n",
    "            'filePattern': csv_file_patterns,\n",
    "            'groupBy': self.user_args['groupBy'],\n",
    "            'outDir': self.workflow['CsvStatistics']\n",
    "        }\n",
    "\n",
    "        self.plugin_args = {\n",
    "            'BasicFlatfieldCorrectionPlugin':  self.basic_flatfield_args,\n",
    "            'ApplyFlatfield': self.apply_flatfield_args,\n",
    "            'DemoSmpTraining_Inference': self.smp_args,\n",
    "            'FtlLabel': self.ftl_args,\n",
    "            'BinaryOperationsPlugin': self.binary_args,\n",
    "            'NyxusPlugin': self.nyxus_args,\n",
    "            'TabularThresholdingPlugin': self.threshold_args,\n",
    "            'CsvStatistics': self.csv_args,\n",
    "            'volume': self.volume,\n",
    "            'run_flatfield': self.run_flatfield.value\n",
    "        }\n",
    "\n",
    "    def _clean(self):\n",
    "        \"\"\"\n",
    "        Permanently deletes all workflow output files.\n",
    "        \"\"\"\n",
    "        shutil.rmtree(self.outDir)\n",
    "        # shutil.rmtree(self.final_dir)\n",
    "        with self.out:\n",
    "            print('Removed {}'.format(self.outDir))\n",
    "            print('Removed {}'.format(self.final_dir))\n",
    "\n",
    "    def _stop_containers(self):\n",
    "        \"\"\"\n",
    "        Stops all the containers associated with the workflow.\n",
    "        \"\"\"\n",
    "        for h in self.hashes:\n",
    "            cmd = 'docker stop {}'.format(h)\n",
    "            stop = subprocess.check_output(cmd, shell=True).decode(sys.stdout.encoding).strip()\n",
    "            with self.out:\n",
    "                print('Stopped container {}'.format(h))\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        A method to start the interactive dashboard.\n",
    "        \"\"\"\n",
    "\n",
    "        display(widgets.interactive(self.display_widgets))\n",
    "\n",
    "\n",
    "# Define and start the dashboard\n",
    "ui = Visualization_UI()\n",
    "ui.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f957b1-43ab-489b-9e38-d4e168502215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "53f2d05ab41ad01012fc99731db73cd0052575fd301b1ff35d89d9b9660a7c8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
