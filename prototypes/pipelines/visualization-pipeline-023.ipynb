{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8045b3cc-f146-4bdd-9ea2-434816142a65",
   "metadata": {},
   "source": [
    "# Visualization Pipeline Version 0.2.3\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Enter inputs, press `Save Inputs` button, check if the inputs were validated. If they were not validated see output explanation. Change input and press `Save Inputs` again. If any variables do not apply to your application, they should be completely empty.\n",
    "- Press `Run Pipeline`, the progress indicator bar at the bottom indicates how many data subsets have been processed.\n",
    "- The UI will display links to view the images of each data subset in AVIATOR.\n",
    "- **Important:** The input and output directories must both be in one of the following `/home/jovyan/shared/data/` (recommended) or `/home/jovyan/shared/notebooks/` or `/home/jovyan/work/`\n",
    "\n",
    "### Input Descriptions\n",
    "- `inpDir`: Full path to the input images, should start with `/home/jovyan/`\n",
    "- `outDir`: Full path to output, if you want to view images in AVIATOR then it should be in `/home/jovyan/shared/data/file-server/`\n",
    "- `logPath`: Full path to this dashboard log outputs\n",
    "- `filePattern`: The file pattern of the input images (Include ALL variables)\n",
    "- `subsetBy`: The variable used to subset the data. For instance, you might process each plate individually to have a final Zarr pyramid for each plate.\n",
    "- `depthVar`: The variable which represents a third dimension as opposed to width and height. This could be channel, time slice etc.\n",
    "- `layout`: How to images should be assembled. This is the montage plugin layout. Please see https://github.com/PolusAI/polus-plugins/tree/master/transforms/images/polus-montage-plugin for more details. Note neither the `depthVar` or `subsetBy` variables can be included in the layout.\n",
    "- `flipAxis`: The vaiable if any which should be flipped when assembling the images. For instance, if the x varible represents the width dimension and the images are flipped from right to left then the input would simple be `x`\n",
    "- `subsets`: Which data subsets to process, for example if you want to process the first 3 plates you can input `1,2,3` or `all` to process all subsets\n",
    "\n",
    "### Version Notes\n",
    "- This version supports parallel execution. Each worker is responsible for starting the workflow (for a given subset) and monitoring its containers so that it can start the next container in the workflow upon completion of the previous container. \n",
    "- This version allows for the center FOV to be used for basic flatfield estimation or all images.\n",
    "- Ability to resume a job by parsing the output directory, there is also functionality to parse the logs in this file but it not currently used.\n",
    "- Ability to select if flatfield correction should be run or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d406de-47af-4d22-b1e9-f93c049a08b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa3969e4bd34af59d765c6a5b35b10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import re\n",
    "import subprocess\n",
    "import ipywidgets as widgets\n",
    "import multiprocessing as mp\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "from filepattern import FilePattern\n",
    "from multiprocessing import Pool\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def parallelize_workflow(map_args, ui, log_path):\n",
    "    hashes = []\n",
    "    num_workers = min(int(mp.cpu_count()//2), 4)\n",
    "    with ui.out:\n",
    "        print('Number of workers = {}'.format(num_workers))\n",
    "        print('Spawning workers...')\n",
    "        complete = 0\n",
    "\n",
    "        with Pool(processes=num_workers) as pool:\n",
    "            for status, subset, subset_hashes in pool.imap(run_workflow, map_args):\n",
    "                hashes.extend(subset_hashes)\n",
    "                with open(log_path, 'a') as fw:\n",
    "                    if status:\n",
    "                        fw.write('{} : subset {} successful : hashes : {}\\n'.format(datetime.now(), subset, ' '.join(subset_hashes)))\n",
    "\n",
    "                        # Stop the successful containers upon completion\n",
    "                        stopped_containers = stop_containers(subset_hashes)\n",
    "                        print('Stopped containers of subset {} : hashes : {}'.format(subset, ' '.join(stopped_containers)))\n",
    "\n",
    "                    else:\n",
    "                        fw.write('ERROR : {} : subset {} failed : hashes : {}\\n'.format(datetime.now(), subset, ' '.join(subset_hashes)))\n",
    "\n",
    "                    fw.close()\n",
    "\n",
    "                complete += 1\n",
    "                ui.progress_bar.value = complete\n",
    "\n",
    "    return hashes\n",
    "\n",
    "\n",
    "def check_container(container_hash):\n",
    "    \"\"\"\n",
    "    Checks the status of a docker container.\n",
    "    \"\"\"\n",
    "    cmd = 'docker ps'\n",
    "    out = subprocess.check_output(cmd, shell=True).decode(sys.stdout.encoding).split('\\n')\n",
    "    containers = [c.split() for c in out]\n",
    "    status = 'Unknown'\n",
    "    # with self.out:\n",
    "    #     print(containers)\n",
    "    for c in containers:\n",
    "        if container_hash in c:\n",
    "            status = c[-2]\n",
    "            break\n",
    "    return status\n",
    "\n",
    "\n",
    "def run_container(args, volume):\n",
    "    \"\"\"\n",
    "    Runs a docker container.\n",
    "    \"\"\"\n",
    "    dock8r = 'docker run -v'\n",
    "\n",
    "    jvolume, dvolume = volume.split(':')\n",
    "\n",
    "    container = args['container']\n",
    "    plugin_args = [\n",
    "        '--' + arg + ' ' + str(value)\n",
    "        for arg, value in args.items() if arg != 'container' and value != ''\n",
    "    ]\n",
    "    for i, arg in enumerate(plugin_args):\n",
    "        if jvolume in arg:\n",
    "            plugin_args[i] = arg.replace(jvolume, dvolume)\n",
    "\n",
    "    cmd_list = [dock8r] + [volume] + [container] + plugin_args\n",
    "    cmd = ' '.join(cmd_list)\n",
    "\n",
    "    full_hash = subprocess.check_output(cmd, shell=True).decode(sys.stdout.encoding).strip()\n",
    "    container_hash = full_hash.split('-')[-1]\n",
    "    print('Running {} hash:{}'.format(container, container_hash))\n",
    "\n",
    "    status = 'Unknown'\n",
    "    i = 0\n",
    "\n",
    "    while status != 'Succeeded':\n",
    "        sleep(5)\n",
    "        status = check_container(container_hash)\n",
    "        print('Status of Container {} is {} : {}'.format(container_hash, status, datetime.now()))\n",
    "        i += 1\n",
    "        if i > 20 and status == 'Pending':\n",
    "            return 'Pending', container_hash\n",
    "\n",
    "        if status == 'Failed':\n",
    "            return 'Failed', container_hash\n",
    "\n",
    "    return 'Succeeded', container_hash\n",
    "\n",
    "\n",
    "def run_workflow(plugin_args):\n",
    "    \"\"\"\n",
    "    Controls the visualization workflow for a data subset.\n",
    "    \"\"\"\n",
    "\n",
    "    hashes = []\n",
    "    volume = plugin_args['volume']\n",
    "    subset = plugin_args['subset']\n",
    "    run_flatfield = plugin_args['run_flatfield']\n",
    "\n",
    "    try:\n",
    "\n",
    "        if run_flatfield:\n",
    "\n",
    "            # Run the basic flatfiled for specified subset\n",
    "            print('Running Basic Flatfield Plugin on subset {}'.format(subset))\n",
    "            basic_flatfield_args = plugin_args['BasicFlatfieldCorrectionPlugin']\n",
    "            basic_flatfield_args['filePattern'] = basic_flatfield_args['filePattern'][subset]\n",
    "            basic_flatfield_args['outDir'] = basic_flatfield_args['outDir'][subset]\n",
    "            # pprint(basic_flatfield_args)\n",
    "            status, container_hash = run_container(basic_flatfield_args, volume)\n",
    "            hashes.append(container_hash)\n",
    "            if status != 'Succeeded':\n",
    "                raise Exception('Status of container {} is {}'.format(\n",
    "                    container_hash, status)\n",
    "                               )\n",
    "\n",
    "            # Run the apply flatfield plugin for specified subset\n",
    "            print('Running Apply Flatfield Plugin on subset {}'.format(subset))\n",
    "            apply_flatfield_args = plugin_args['ApplyFlatfield']\n",
    "            apply_flatfield_args['imgPattern'] = apply_flatfield_args['imgPattern'][subset]\n",
    "            apply_flatfield_args['ffDir'] = basic_flatfield_args['outDir']\n",
    "            apply_flatfield_args['brightPattern'] = apply_flatfield_args['brightPattern'][subset]\n",
    "            apply_flatfield_args['darkPattern'] = apply_flatfield_args['darkPattern'][subset]\n",
    "            apply_flatfield_args['outDir'] = apply_flatfield_args['outDir'][subset]\n",
    "            # pprint(apply_flatfield_args)\n",
    "            status, container_hash = run_container(apply_flatfield_args, volume)\n",
    "            hashes.append(container_hash)\n",
    "            if status != 'Succeeded':\n",
    "                raise Exception('Status of container {} is {}'.format(\n",
    "                    container_hash, status)\n",
    "                               )\n",
    "\n",
    "        else:\n",
    "            basic_flatfield_args = plugin_args['BasicFlatfieldCorrectionPlugin']\n",
    "            apply_flatfield_args = plugin_args['ApplyFlatfield']\n",
    "            apply_flatfield_args['outDir'] = basic_flatfield_args['inpDir']\n",
    "\n",
    "        # Run the montage plugin for specified subset\n",
    "        print('Running Montage Plugin on subset {}'.format(subset))\n",
    "        montage_args = plugin_args['Montage']\n",
    "        montage_args['inpDir'] = apply_flatfield_args['outDir']\n",
    "        montage_args['filePattern'] = montage_args['filePattern'][subset]\n",
    "        montage_args['outDir'] = montage_args['outDir'][subset]\n",
    "        # pprint(montage_args)\n",
    "        status, container_hash = run_container(montage_args, volume)\n",
    "        hashes.append(container_hash)\n",
    "        if status != 'Succeeded':\n",
    "            raise Exception('Status of container {} is {}'.format(\n",
    "                container_hash, status)\n",
    "                           )\n",
    "        # Run the assembler plugin for specified subset\n",
    "        print('Running Image Assembler Plugin on subset {}'.format(subset))\n",
    "        assembler_args = plugin_args['ImageAssembler']\n",
    "        assembler_args['imgPath'] = apply_flatfield_args['outDir']\n",
    "        assembler_args['stitchPath'] = montage_args['outDir']\n",
    "        assembler_args['outDir'] = assembler_args['outDir'][subset]\n",
    "        # pprint(assembler_args)\n",
    "        status, container_hash = run_container(assembler_args, volume)\n",
    "        status, hashes.append(container_hash)\n",
    "        if status != 'Succeeded':\n",
    "            raise Exception('Status of container {} is {}'.format(\n",
    "                container_hash, status)\n",
    "                           )\n",
    "\n",
    "        # Run the precompute plugin for specified subset\n",
    "        print('Running Precompute Slide Plugin on subset {}'.format(subset))\n",
    "        precompute_args = plugin_args['PolusPrecomputeSlidePlugin']\n",
    "        precompute_args['inpDir'] = assembler_args['outDir']\n",
    "        precompute_args['filePattern'] = precompute_args['filePattern'][subset]\n",
    "        precompute_args['outDir'] = precompute_args['outDir'][subset]\n",
    "        # pprint(precompute_args)\n",
    "        status, container_hash = run_container(precompute_args, volume)\n",
    "        hashes.append(container_hash)\n",
    "        if status != 'Succeeded':\n",
    "            raise Exception('Status of container {} is {}'.format(\n",
    "                container_hash, status)\n",
    "                           )\n",
    "\n",
    "        return True, subset, hashes\n",
    "\n",
    "    except Exception:\n",
    "        print('Subset {} failed'.format(subset))\n",
    "        return False, subset, hashes\n",
    "\n",
    "\n",
    "def common_parent(paths):\n",
    "    \"\"\"\n",
    "    Returns the deepest common parent of a list of paths.\n",
    "    \"\"\"\n",
    "    parents = [\n",
    "        [parent for parent in Path(path).parents][:-1]\n",
    "        for path in paths]\n",
    "\n",
    "    for parent_list in parents:\n",
    "        parent_list.reverse()\n",
    "\n",
    "    common = None\n",
    "\n",
    "    for level in zip(*parents):\n",
    "        if len(set(level)) == 1:\n",
    "            common = level[0]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return common\n",
    "\n",
    "\n",
    "def stop_containers(hashes):\n",
    "    \"\"\"\n",
    "    Stops all specified containers.\n",
    "    \"\"\"\n",
    "    stopped_containers = []\n",
    "    for h in hashes:\n",
    "        cmd = 'docker stop {}'.format(h)\n",
    "        stopped_containers.append(\n",
    "            subprocess.check_output(\n",
    "                cmd, shell=True\n",
    "            ).decode(sys.stdout.encoding).strip()\n",
    "        )\n",
    "        # print('Stopped container {}'.format(h))\n",
    "\n",
    "    return stopped_containers\n",
    "\n",
    "\n",
    "def parse_log(log_path):\n",
    "\n",
    "    re_subset = re.compile(r'(?<=subset\\s)[0-9]+')\n",
    "    success = {}\n",
    "\n",
    "    with open(log_path) as fr:\n",
    "        for line in fr.readlines():\n",
    "\n",
    "            subset = int(re_subset.search(line).group(0))\n",
    "\n",
    "            if 'ERROR' in line:\n",
    "                success[subset] = False\n",
    "\n",
    "            else:\n",
    "                success[subset] = True\n",
    "\n",
    "    complete_subsets = [subset for subset, status in success.items() if status]\n",
    "    failed_subsets = [subset for subset, status in success.items() if not status]\n",
    "\n",
    "    return complete_subsets, failed_subsets\n",
    "\n",
    "\n",
    "def parse_output(outDir):\n",
    "\n",
    "    complete = []\n",
    "    incomplete = []\n",
    "    precompute_out = outDir.joinpath('PolusPrecomputeSlidePlugin')\n",
    "\n",
    "    for subset in precompute_out.iterdir():\n",
    "\n",
    "        s = int(''.join([i for i in str(subset.name) if i.isnumeric()]))\n",
    "\n",
    "        if list(subset.iterdir()):\n",
    "            complete.append(s)\n",
    "\n",
    "        else:\n",
    "            incomplete.append(s)\n",
    "\n",
    "    complete.sort()\n",
    "    incomplete.sort()\n",
    "\n",
    "    return complete, incomplete\n",
    "\n",
    "\n",
    "class Visualization_UI:\n",
    "    \"\"\"\n",
    "    A class to display a UI of the visualization pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Define dashboard style and stdout\n",
    "        self.layout = widgets.Layout(width='auto', height='40px')\n",
    "        self.style = {'description_width': 'initial'}\n",
    "        self.out = widgets.Output(layout={'border': '1px solid black'})\n",
    "\n",
    "        # Define variable to track dashboard workflow\n",
    "        self.running = False\n",
    "        self.valid_inputs = False\n",
    "\n",
    "        self.hashes = []\n",
    "\n",
    "        # Define the pipeline plugin workflow\n",
    "        self.workflow = {\n",
    "            'BasicFlatfieldCorrectionPlugin': None,\n",
    "            'ApplyFlatfield': None,\n",
    "            'Montage': None,\n",
    "            'ImageAssembler': None,\n",
    "            'PolusPrecomputeSlidePlugin': None,\n",
    "            'smp-training': None\n",
    "        }\n",
    "\n",
    "        # Define the user arguments required for the pipeline\n",
    "        self.user_args = {\n",
    "            'inpDir': '/home/jovyan/shared/data/eastman_subset/input/',\n",
    "            'outDir': '/home/jovyan/shared/data/file-server/pyramids/visualization-pipeline/test-v023/',\n",
    "            'logPath': '/home/jovyan/shared/notebooks/pipelines/visualization/logs/test-v023.log',\n",
    "            'filePattern': 'p{ppp}_x{xx}_y{yy}_wx{t}_wy{z}_c{c}.ome.tif',\n",
    "            'wellVar': 'tz',\n",
    "            'subsetBy': 'p',\n",
    "            'depthVar': 'c',\n",
    "            'layout': 'tz,xy',\n",
    "            'flipAxis': 'z',\n",
    "            'subsets': 'all',\n",
    "        }\n",
    "\n",
    "        # Define the pipeline's user input widgets\n",
    "        self.arg_widgets = [\n",
    "            widgets.Text(\n",
    "                value=self.user_args[f],\n",
    "                description=f,\n",
    "                layout=self.layout,\n",
    "                style=self.style\n",
    "            )\n",
    "            for f in self.user_args.keys()\n",
    "        ]\n",
    "\n",
    "        # Define ability for user to run flatfield or not\n",
    "        self.run_flatfield = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description='Run Flatfield Correction',\n",
    "            disabled=False\n",
    "        )\n",
    "\n",
    "        self.pyramid_type = widgets.RadioButtons(\n",
    "            options=['image', 'segmentation'],\n",
    "            value='image',\n",
    "            description='Image Type:',\n",
    "            disabled=False\n",
    "        )\n",
    "\n",
    "        self.center_fov = widgets.RadioButtons(\n",
    "            options=['normal', 'center fov'],\n",
    "            value='normal',\n",
    "            description='Flatfield Correction:',\n",
    "            disabled=False,\n",
    "            layout=self.layout,\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "\n",
    "        # Define the save inputs button\n",
    "        self.save_args = widgets.Button(\n",
    "            value=False,\n",
    "            description='Save Inputs',\n",
    "            disabled=False,\n",
    "            button_style='',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "            tooltip='Save the Inputs',\n",
    "            icon='save'  # (FontAwesome names without the `fa-` prefix)\n",
    "        )\n",
    "\n",
    "        self.run_pipeline = widgets.Button(\n",
    "            value=True,\n",
    "            description='Run Pipeline',\n",
    "            disabled=False,\n",
    "            button_style='',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "            tooltip='Runs the pipeline with current inputs (make sure to save inputs first)',\n",
    "            icon='play'  # (FontAwesome names without the `fa-` prefix)\n",
    "        )\n",
    "\n",
    "        self.resume_pipeline = widgets.Button(\n",
    "            value=True,\n",
    "            description='Resume Pipeline',\n",
    "            disabled=False,\n",
    "            button_style='',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "            tooltip='Resumes the pipeline with current inputs (make sure to save inputs first)',\n",
    "            icon='step-forward'  # (FontAwesome names without the `fa-` prefix)\n",
    "        )\n",
    "\n",
    "        self.progress_bar = widgets.IntProgress(\n",
    "            value=0,\n",
    "            min=0,\n",
    "            max=None,\n",
    "            description='Subsets Processed:',\n",
    "            style=dict(description_width='initial'),\n",
    "            layout=self.layout\n",
    "            )\n",
    "\n",
    "        # Define the method to run on button clicks\n",
    "        self.save_args.on_click(self.on_save_clicked)\n",
    "        self.run_pipeline.on_click(self.on_run_clicked)\n",
    "        self.resume_pipeline.on_click(self.on_resumed_clicked)\n",
    "\n",
    "        self.w = [\n",
    "            *self.arg_widgets,\n",
    "            self.run_flatfield,\n",
    "            self.center_fov,\n",
    "            self.pyramid_type,\n",
    "            self.save_args,\n",
    "            self.run_pipeline,\n",
    "            self.resume_pipeline,\n",
    "            self.progress_bar,\n",
    "            self.out\n",
    "        ]\n",
    "\n",
    "    def display_widgets(self):\n",
    "        \"\"\"\n",
    "        A method to display the widgets\n",
    "        \"\"\"\n",
    "\n",
    "        display(\n",
    "            *self.w\n",
    "        )\n",
    "\n",
    "    def on_save_clicked(self, b):\n",
    "        \"\"\"\n",
    "        A method to save the users inputs.\n",
    "        \"\"\"\n",
    "        with self.out:\n",
    "            print('Saving args...')\n",
    "            for w in self.arg_widgets:\n",
    "                arg = w.description\n",
    "                value = w.value\n",
    "                self.user_args[arg] = value\n",
    "                print('{} = {}'.format(arg, value))\n",
    "\n",
    "            if self.run_flatfield.value:\n",
    "                if self.center_fov.value == 'normal':\n",
    "                    print('You have selected to run a normal Flatfield Correction')\n",
    "                else:\n",
    "                    print('You have selected to run a center FOV Flatfield Correction')\n",
    "\n",
    "            else:\n",
    "                print('You have selected to NOT run Flatfield Correction')\n",
    "\n",
    "            if self.pyramid_type.value == 'image':\n",
    "                print('You have indicated you have a non-segmented image')\n",
    "\n",
    "            else:\n",
    "                print('You have indicated you have a segmented image')\n",
    "\n",
    "            print('Validating inputs...')\n",
    "            self.valid_inputs = self.validate_inputs()\n",
    "\n",
    "    def validate_inputs(self):\n",
    "        \"\"\"\n",
    "        A method to validate the user's inputs.\n",
    "        \"\"\"\n",
    "        with self.out:\n",
    "            inpDir = Path(self.user_args['inpDir'])\n",
    "            outDir = Path(self.user_args['outDir'])\n",
    "\n",
    "            # Validate inpDir\n",
    "            if not inpDir.exists():\n",
    "                print('\\n***WARNING***\\nThe inpDir {} does not exist'.format(inpDir))\n",
    "                return False  \n",
    "\n",
    "            # TODO: Make sure inpDir and outDir either in:\n",
    "            # /shared/data/ or /shared/notebooks/ or /work/\n",
    "\n",
    "            if self.user_args['subsetBy'] in self.user_args['layout']:\n",
    "                print('The subsetBy variable cannot be in the layout input')\n",
    "                return False\n",
    "\n",
    "            if self.user_args['depthVar'] in self.user_args['layout']:\n",
    "                print('The depthVar variable cannot be in the layout input')\n",
    "                return False\n",
    "\n",
    "            # Transform layout variable to correct format\n",
    "            layout = self.user_args['layout']\n",
    "            layout = [g.strip() for g in layout.split(',')]\n",
    "\n",
    "            # Check each layout grouping length\n",
    "            for g in layout:\n",
    "                if len(g) > 2:\n",
    "                    print('Each layout group must be at most 2 variables')\n",
    "                    return False\n",
    "            self.user_args['layout'] = ','.join(layout)\n",
    "\n",
    "            print('Inputs validated!')\n",
    "            return True\n",
    "\n",
    "    def on_run_clicked(self, b):\n",
    "        \"\"\"\n",
    "        A method to start the pipeline on click\n",
    "        \"\"\" \n",
    "\n",
    "        if self.running:\n",
    "            with self.out:\n",
    "                print('Pipeline already running, you must kill the pipeline to restart')\n",
    "\n",
    "        elif not self.valid_inputs:\n",
    "            with self.out:\n",
    "                print('The inputs must be validated before running the pipeline, click Save Inputs to validate')\n",
    "\n",
    "        else:\n",
    "            self.running = True\n",
    "\n",
    "            with self.out:\n",
    "                print('Starting the pipeline...')\n",
    "                print('Inferring plugin inputs and creating output file locations')\n",
    "                self.generate_plugin_inputs()\n",
    "                print('Running containers...')\n",
    "\n",
    "            self.subsets = [i for i in range(len(self.filePatterns))]\n",
    "            map_args = [self.plugin_args.copy() for subset in self.subsets]\n",
    "\n",
    "            for subset in self.subsets:\n",
    "                map_args[subset]['subset'] = subset\n",
    "\n",
    "            if self.user_args['subsets'] != 'all':\n",
    "                to_process = [int(s) for s in self.user_args['subsets'].split(',')]\n",
    "                map_args = [map_args[s] for s in to_process]\n",
    "\n",
    "            self.progress_bar.max = len(map_args)\n",
    "            log_path = self.user_args['logPath']\n",
    "\n",
    "            self.hashes.extend(parallelize_workflow(map_args, self, log_path))\n",
    "\n",
    "            self.running = False\n",
    "\n",
    "            if 'file-server' in self.user_args['outDir']:\n",
    "                with self.out:\n",
    "                    links = self.generate_aviator()\n",
    "                    print('AVIATOR Links:')\n",
    "                    for l in links:\n",
    "                        print(l)\n",
    "\n",
    "    def on_resumed_clicked(self, b):\n",
    "        \"\"\"\n",
    "        A method to resume the pipeline\n",
    "        \"\"\"\n",
    "\n",
    "        out_dir = Path(self.user_args['outDir'])\n",
    "\n",
    "        with self.out:\n",
    "            assert out_dir.exists(), 'The output directory for resuming a pipeline must already exist'\n",
    "\n",
    "        if self.running:\n",
    "            with self.out:\n",
    "                print('Pipeline already running, you must kill the pipeline to restart')\n",
    "\n",
    "        elif not self.valid_inputs:\n",
    "            with self.out:\n",
    "                print('The inputs must be validated before running the pipeline, click Save Inputs to validate')\n",
    "\n",
    "        else:\n",
    "            self.running = True\n",
    "\n",
    "            with self.out:\n",
    "                log_path = self.user_args['logPath']\n",
    "                print('Parsing {}'.format(out_dir))\n",
    "\n",
    "                # complete_subsets, failed_subsets = parse_log(log_path)\n",
    "                complete_subsets, failed_subsets = parse_output(out_dir)\n",
    "\n",
    "                print('Inferring plugin inputs and output file locations...')\n",
    "                self.generate_plugin_inputs(resume=True)\n",
    "\n",
    "                map_args = []\n",
    "                self.subsets = []\n",
    "\n",
    "                if self.user_args['subsets'] != 'all':\n",
    "                    user_subsets = [int(s) for s in self.user_args['subsets'].split(',')]\n",
    "\n",
    "                for i in range(len(self.filePatterns)):\n",
    "                    if i not in complete_subsets:\n",
    "                        if self.user_args['subsets'] == 'all' or i in user_subsets:\n",
    "                            plugin_args = self.plugin_args.copy()\n",
    "                            plugin_args['subset'] = i\n",
    "                            map_args.append(plugin_args)\n",
    "                            self.subsets .append(i)\n",
    "\n",
    "                self.progress_bar.max = len(map_args)\n",
    "                log_path = self.user_args['logPath']\n",
    "\n",
    "                print('Resuming subsets : {}'.format(self.subsets))\n",
    "                print('Starting containers...')\n",
    "                self.hashes.extend(parallelize_workflow(map_args, self, log_path))\n",
    "\n",
    "                self.running = False\n",
    "\n",
    "                if 'file-server' in self.user_args['outDir']:\n",
    "                    with self.out:\n",
    "                        links = self.generate_aviator()\n",
    "                        print('AVIATOR Links:')\n",
    "                        for l in links:\n",
    "                            print(l)\n",
    "\n",
    "    def generate_outdir(self, resume):\n",
    "\n",
    "        self.outDir = Path(self.user_args['outDir'])\n",
    "\n",
    "        if not resume:\n",
    "            os.mkdir(self.outDir)\n",
    "\n",
    "        flatfield_plugins = [\n",
    "            'BasicFlatfieldCorrectionPlugin', 'ApplyFlatfield'\n",
    "        ]\n",
    "\n",
    "        # Make plugin out directories\n",
    "        for plugin in self.workflow:\n",
    "\n",
    "            if plugin in flatfield_plugins and not self.run_flatfield.value:\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                sub_dir = self.outDir.joinpath(plugin)\n",
    "\n",
    "            if not resume:\n",
    "                os.mkdir(sub_dir)\n",
    "\n",
    "            self.workflow[plugin] = []\n",
    "\n",
    "            # For each data subset create a subset directory\n",
    "            for i in range(len(self.filePatterns)):\n",
    "\n",
    "                subset_dir = sub_dir.joinpath('subset{}'.format(str(i)))\n",
    "                self.workflow[plugin].append(subset_dir)\n",
    "\n",
    "                if resume:\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    os.mkdir(subset_dir)\n",
    "\n",
    "    # TODO: Assemble patterns should be gerneated from actual file list\n",
    "    def generate_assemble_pattern(self, groupBy):\n",
    "        \"\"\"\n",
    "        Generates assembled patterns, similair to a vector pattern\n",
    "        \"\"\"\n",
    "        self.assemblePatterns = self.filePatterns.copy()\n",
    "        self.assembleFlatfieldPatterns = self.flatfieldPatterns.copy()\n",
    "\n",
    "        # Iterate over the groupBy variables\n",
    "        for v in groupBy:\n",
    "            # Define the re pattern\n",
    "            re_pattern = re.compile(r'\\{(' + v + r'+)\\}')\n",
    "            # Define the min and max unique values of the variable\n",
    "            v_unique = self.fp.uniques[v]\n",
    "            v_min = min(v_unique)\n",
    "            v_max = max(v_unique)\n",
    "            num_dig = len(str(v_max))\n",
    "            # Define the pattern to sub in for the variable\n",
    "            v_pattern = '(' + str(v_min).zfill(num_dig) + '-' + str(v_max) + ')'\n",
    "\n",
    "            # Iterate over all file patterns (one for each data subset)\n",
    "            for i, pattern in enumerate(self.assemblePatterns):\n",
    "                assemblePattern = re_pattern.sub(v_pattern, pattern)\n",
    "                self.assemblePatterns[i] = assemblePattern\n",
    "\n",
    "            # Iterate over all file patterns (one for each data subset)\n",
    "            for i, pattern in enumerate(self.assembleFlatfieldPatterns):\n",
    "                assemblePattern = re_pattern.sub(v_pattern, pattern)\n",
    "                self.assembleFlatfieldPatterns[i] = assemblePattern\n",
    "\n",
    "        self.assemblePatterns = ['\"' + p + '\"' for p in self.assemblePatterns]\n",
    "        self.assembleFlatfieldPatterns = ['\"' + p + '\"' for p in self.assembleFlatfieldPatterns]\n",
    "\n",
    "    def generate_plugin_inputs(self, resume=False):\n",
    "\n",
    "        self.fp = FilePattern(\n",
    "            self.user_args['inpDir'], self.user_args['filePattern']\n",
    "        )\n",
    "\n",
    "        flatfield_pattern = self.user_args['filePattern']\n",
    "\n",
    "        if self.center_fov.value != 'normal':\n",
    "            # Find range of well variables\n",
    "            for v in self.user_args['wellVar']:\n",
    "                # Define the regular expression to parse the well variables\n",
    "                well_re = re.compile(r'\\{(' + v + r'+)\\}')\n",
    "                uniques = self.fp.uniques[v]\n",
    "                idx = len(uniques)//2\n",
    "                flatfield_pattern = well_re.sub(str(uniques[idx]), flatfield_pattern)\n",
    "\n",
    "        if self.user_args['subsetBy']:\n",
    "\n",
    "            # Define the variable which the data will be split\n",
    "            v = self.user_args['subsetBy']\n",
    "\n",
    "            # Define the regular expression to parse the subset variable\n",
    "            subset_re = re.compile(r'\\{(' + v + r'+)\\}')\n",
    "            pattern = self.user_args['filePattern']\n",
    "\n",
    "            # Find the subset variable and its length\n",
    "            subset_var = subset_re.search(pattern).group(1)\n",
    "            subset_len = len(subset_var)\n",
    "\n",
    "            # Get the subset images\n",
    "            # subsets = self.subset_images()\n",
    "\n",
    "            # Generate the filepatterns for each data subset\n",
    "            self.filePatterns = [\n",
    "                subset_re.sub(str(value).zfill(subset_len), pattern)\n",
    "                for value in self.fp.uniques[v]\n",
    "            ]\n",
    "\n",
    "            self.flatfieldPatterns = [\n",
    "                subset_re.sub(str(value).zfill(subset_len), flatfield_pattern)\n",
    "                for value in self.fp.uniques[v]\n",
    "            ]\n",
    "\n",
    "            self.filePatterns.sort()\n",
    "            self.flatfieldPatterns.sort()\n",
    "            groupBy = self.fp.variables.replace(v, '')\n",
    "\n",
    "        else:\n",
    "            self.filePatterns = [self.fp]\n",
    "            groupBy = self.fp.varaibles\n",
    "\n",
    "        if self.user_args['depthVar']:\n",
    "            groupBy = groupBy.replace(self.user_args['depthVar'], '')\n",
    "\n",
    "        with self.out:\n",
    "            print('Generating assembled file patterns')\n",
    "        # Generate the assembled patterns for each subset\n",
    "        self.generate_assemble_pattern(groupBy)\n",
    "\n",
    "        with self.out:\n",
    "            print('Generating output directories')\n",
    "        self.generate_outdir(resume)\n",
    "\n",
    "        # Define the contianer volumes\n",
    "        # jvolume = str(common_parent([ui.outDir, ui.final_dir]))\n",
    "        jvolume = '/' + '/'.join(common_parent([ui.outDir]).parts[1:5]) + '/'\n",
    "        dvolume = jvolume.replace('/home/jovyan', '/opt')\n",
    "        self.volume = jvolume + ':' + dvolume\n",
    "\n",
    "        # Define the plugin inputs\n",
    "        self.basic_flatfield_args = {\n",
    "            'container': 'polusai/basic-flatfield-correction-plugin:1.3.1',\n",
    "            'inpDir': self.user_args['inpDir'],\n",
    "            'filePattern': self.flatfieldPatterns,\n",
    "            'darkfield': True,\n",
    "            'photobleach': True,\n",
    "            'groupBy': groupBy,\n",
    "            'outDir': self.workflow['BasicFlatfieldCorrectionPlugin']\n",
    "        }\n",
    "\n",
    "        brightPatterns = [pattern.replace('.ome.tif', '_flatfield.ome.tif')\n",
    "                          for pattern in self.assembleFlatfieldPatterns]\n",
    "\n",
    "        darkPatterns = [pattern.replace('.ome.tif', '_darkfield.ome.tif')\n",
    "                        for pattern in self.assembleFlatfieldPatterns]\n",
    "\n",
    "        self.apply_flatfield_args = {\n",
    "            'container': 'polusai/apply-flatfield-plugin:1.2.0',\n",
    "            'imgDir': self.user_args['inpDir'],\n",
    "            'imgPattern': self.filePatterns,\n",
    "            'brightPattern': brightPatterns,\n",
    "            'darkPattern': darkPatterns,\n",
    "            'outDir': self.workflow['ApplyFlatfield']\n",
    "        }\n",
    "\n",
    "        self.montage_args = {\n",
    "            'container': 'polusai/montage-plugin:0.4.0',\n",
    "            'filePattern': self.filePatterns,\n",
    "            'layout': self.user_args['layout'],\n",
    "            'imageSpacing': '1',\n",
    "            'gridSpacing': '20',\n",
    "            'flipAxis': self.user_args['flipAxis'],\n",
    "            'outDir': self.workflow['Montage']\n",
    "        }\n",
    "\n",
    "        self.assembler_args = {\n",
    "            'container': 'polusai/image-assembler-plugin:1.1.5',\n",
    "            'timesliceNaming': True,\n",
    "            'outDir': self.workflow['ImageAssembler']\n",
    "        }\n",
    "\n",
    "        self.precompute_args = {\n",
    "            'container': 'polusai/precompute-slide-plugin:1.5.0',\n",
    "            'pyramidType': 'Zarr',\n",
    "            'filePattern': self.assemblePatterns,\n",
    "            'outDir': self.workflow['PolusPrecomputeSlidePlugin']\n",
    "        }\n",
    "\n",
    "        if self.pyramid_type == 'image':\n",
    "            self.precompute_args['imageType'] = 'image'\n",
    "\n",
    "        else:\n",
    "            self.precompute_args['imageType'] = 'segmentation'\n",
    "\n",
    "        self.plugin_args = {\n",
    "            'BasicFlatfieldCorrectionPlugin':  self.basic_flatfield_args,\n",
    "            'ApplyFlatfield': self.apply_flatfield_args,\n",
    "            'Montage': self.montage_args,\n",
    "            'ImageAssembler': self.assembler_args,\n",
    "            'PolusPrecomputeSlidePlugin': self.precompute_args,\n",
    "            'volume': self.volume,\n",
    "            'run_flatfield': self.run_flatfield.value\n",
    "        }\n",
    "\n",
    "    def generate_aviator(self):\n",
    "        \"\"\"\n",
    "        Generates an AVIATOR link for each data subset.\n",
    "        \"\"\"\n",
    "        links = []\n",
    "        base = 'https://avivator.gehlenborglab.org/?image_url=https://files.scb-ncats.io/pyramids/'\n",
    "\n",
    "        for subset in self.workflow['PolusPrecomputeSlidePlugin']:\n",
    "            for img in subset.iterdir():\n",
    "                parts = list(img.parts)\n",
    "                pyramid_index = parts.index('pyramids')\n",
    "                path = '/'.join(parts[pyramid_index+1:])\n",
    "                link = base + '/' + path + '/'\n",
    "                links.append(link)\n",
    "\n",
    "        return links\n",
    "\n",
    "    def _clean(self):\n",
    "        \"\"\"\n",
    "        Permanently deletes all workflow output files.\n",
    "        \"\"\"\n",
    "        shutil.rmtree(self.outDir)\n",
    "        # shutil.rmtree(self.final_dir)\n",
    "        with self.out:\n",
    "            print('Removed {}'.format(self.outDir))\n",
    "            # print('Removed {}'.format(self.final_dir))\n",
    "\n",
    "    def _stop_containers(self):\n",
    "        \"\"\"\n",
    "        Stops all the containers associated with the workflow.\n",
    "        \"\"\"\n",
    "        for h in self.hashes:\n",
    "            cmd = 'docker stop {}'.format(h)\n",
    "            stop = subprocess.check_output(cmd, shell=True).decode(sys.stdout.encoding).strip()\n",
    "            with self.out:\n",
    "                print('Stopped container {}'.format(h))\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        A method to start the interactive dashboard.\n",
    "        \"\"\"\n",
    "\n",
    "        display(widgets.interactive(self.display_widgets))\n",
    "\n",
    "\n",
    "# Define and start the dashboard\n",
    "ui = Visualization_UI()\n",
    "ui.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b288ef-a117-4776-aec7-cbc39c8f545d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
