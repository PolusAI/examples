{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04f89cd0-839a-4cfa-bb6d-c1b6453d6624",
   "metadata": {},
   "source": [
    "# Pet Breeds Example\n",
    "\n",
    "[Source](https://github.com/fastai/fastbook/blob/master/05_pet_breeds.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cedc64-20bf-4fbf-a452-0ba488078442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "#hide\n",
    "! [ -e /content ] && pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9ebb35-f49e-4ff9-938f-df2f633ac8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e19acf0-9a3c-4b12-92da-a4bf78a3a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "path = untar_data(URLs.PETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59299296-92fa-4bd1-bbe0-4d9c0817928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "Path.BASE_PATH = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ea094-4b68-4fca-a9b0-20100412c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a2c775-27ea-4a58-a665-1d4b987ae26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ffb5ff-e85f-4312-918e-4a7c359d3d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "fastprogress.MAX_COLS = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a15665f-d313-421e-97e2-1e78daead047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(bs, workers=None):\n",
    "    path = untar_data(URLs.PETS)\n",
    "    #source = untar_data(path)\n",
    "    if workers is None: workers = min(8, num_cpus())\n",
    "    batch_tfms = [aug_transforms(size=224, min_scale=0.75)]\n",
    "    dblock = DataBlock(blocks = (ImageBlock, CategoryBlock),\n",
    "                       get_items=get_image_files, \n",
    "                       splitter=RandomSplitter(seed=42),\n",
    "                       get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n",
    "                       item_tfms=Resize(460),\n",
    "                       batch_tfms=aug_transforms(size=224, min_scale=0.75))\n",
    "    return dblock.dataloaders(path/\"images\", bs=bs, num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45556c5-7f80-47d3-b292-34bf1b840a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "@call_parse\n",
    "def main(\n",
    "    gpu:   Param(\"GPU to run on\", int)=None,\n",
    "    bs:    Param(\"Batch size\", int)=64,\n",
    "    arch:  Param(\"Architecture\", str)=resnet34,\n",
    "    runs:  Param(\"Number of times to repeat training\", int)=1\n",
    "):\n",
    "    \"Training of pets.\"\n",
    "    \n",
    "    # gpu = setup_distrib(gpu)\n",
    "    if gpu is not None: torch.cuda.set_device(gpu)\n",
    "        \n",
    "    dls = get_dls(bs)\n",
    "\n",
    "    for run in range(runs):\n",
    "        print(f'Run: {run}')\n",
    "        \n",
    "        learn = cnn_learner(dls, arch, metrics=error_rate).to_fp16()\n",
    "\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "        \n",
    "        # The old way to use DataParallel, or DistributedDataParallel training:\n",
    "        # if gpu is None and n_gpu: learn.to_parallel()\n",
    "        # if num_distrib()>1: learn.to_distributed(gpu) # Requires `-m fastai2.launch`\n",
    "\n",
    "        # the context manager way of dp/ddp, both can handle single GPU base case.\n",
    "        ctx = learn.parallel_ctx if gpu is None and n_gpu else learn.distrib_ctx\n",
    "\n",
    "        with partial(ctx, gpu)(): # distributed traing requires \"-m fastai2.launch\"\n",
    "            print(f\"Training in {ctx.__name__} context on GPU {gpu if gpu is not None else list(range(n_gpu))}\")\n",
    "            learn.fine_tune(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Data Science 0.1.5",
   "language": "python",
   "name": "python-data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
